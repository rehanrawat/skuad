{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "import os.path\n",
    "\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSheets:\n",
    "    def __init__(self) -> None:\n",
    "        self.creds = None\n",
    "        SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\"]\n",
    "\n",
    "        if os.path.exists(\"token.json\"):\n",
    "            self.creds = Credentials.from_authorized_user_file(\"token.json\", SCOPES)\n",
    "        if not self.creds or not self.creds.valid:\n",
    "            if self.creds and self.creds.expired and self.creds.refresh_token:\n",
    "                self.creds.refresh(Request())\n",
    "            else:\n",
    "                flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                \"credentials.json\", SCOPES\n",
    "                )\n",
    "\n",
    "            self.creds = flow.run_local_server(port=0)\n",
    "            with open(\"token.json\", \"w\") as token:\n",
    "                token.write(self.creds.to_json())\n",
    "\n",
    "    def create(self, title):\n",
    "        try:\n",
    "            service = build(\"sheets\", \"v4\", credentials=self.creds)\n",
    "            spreadsheet = {\n",
    "                'properties': {'title': title}\n",
    "            }\n",
    "            spreadsheet = service.spreadsheets().create(body=spreadsheet, fields='spreadsheetId').execute()\n",
    "            return spreadsheet.get('spreadsheetId')\n",
    "\n",
    "        except HttpError as err:\n",
    "            print(err)\n",
    "            return err\n",
    "    def batch_update_values(self, spreadsheet_id, range_name, value_input_options, values):\n",
    "        try:\n",
    "            service = build(\"sheets\", \"v4\", credentials=self.creds)\n",
    "            body={\n",
    "                'valueInputOption':value_input_options,\n",
    "                'data':[{\n",
    "                    'range':range_name,\n",
    "                    'values':values\n",
    "                }]\n",
    "            }\n",
    "            result=service.spreadsheets().values().batchUpdate(spreadsheetId=spreadsheet_id, body=body).execute()\n",
    "            print('{} cells updated.', format(result.get(\"totalUpdatedCells\")))\n",
    "            return result\n",
    "        except HttpError as err:\n",
    "            print(err)\n",
    "            return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkedJobsSpider(scrapy.Spider):\n",
    "    name = \"linkedin\"\n",
    "    api_url = 'https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=&location=Worldwide&locationId=&geoId=92000000&f_TPR=&f_JT=F%2CC&f_WT=2&start=' \n",
    "\n",
    "    def start_requests(self):\n",
    "        first_job_on_page = 0\n",
    "        first_url = self.api_url + str(first_job_on_page)\n",
    "        yield scrapy.Request(url=first_url, callback=self.parse_job, meta={'first_job_on_page': first_job_on_page})\n",
    "\n",
    "    def parse_job(self, response):\n",
    "        first_job_on_page = response.meta['first_job_on_page']\n",
    "\n",
    "        job_item = {}\n",
    "        jobs = response.css(\"li\")\n",
    "\n",
    "        num_jobs_returned = len(jobs)\n",
    "\n",
    "        sheet = GSheets()\n",
    "        spreadsheet_key = '1aD781ZK6pUd5N46fyaXh47D0cZccPPTh_bsL05-OuME'\n",
    "        sid = sheet.open(spreadsheet_key)\n",
    "        \n",
    "        for job in jobs:\n",
    "            company_location = job.css('.job-search-card__location::text').get(default='not-found').strip()\n",
    "            if \"United States\" not in company_location or \"Canada\" not in company_location:\n",
    "                job_item['job_title'] = job.css(\"h3::text\").get(default='not-found').strip()\n",
    "                job_item['job_detail_url'] = job.css(\".base-card__full-link::attr(href)\").get(default='not-found').strip()\n",
    "                job_item['job_listed'] = job.css('time::text').get(default='not-found').strip()\n",
    "                job_item['company_image'] = job.css('div img::attr(data-delayed-url)').get(default='not-found')\n",
    "                job_item['company_name'] = job.css('h4 a::text').get(default='not-found').strip()\n",
    "                job_item['company_link'] = job.css('h4 a::attr(href)').get(default='not-found')\n",
    "                job_item['company_location'] = job.css('.job-search-card__location::text').get(default='not-found').strip()\n",
    "                sheet.batch_update_values(sid, 'A1:A10', 'RAW', [[value for key, value in job_item.items()]])\n",
    "                yield job_item\n",
    "            \n",
    "        if num_jobs_returned > 0:\n",
    "            first_job_on_page = int(first_job_on_page) + 25\n",
    "            next_url = self.api_url + str(first_job_on_page)\n",
    "            yield scrapy.Request(url=next_url, callback=self.parse_job, meta={'first_job_on_page': first_job_on_page})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
